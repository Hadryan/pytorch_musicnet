{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, signal, sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import conv1d\n",
    "import torchvision\n",
    "\n",
    "from time import time\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "import musicnet\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'   # see issue #152\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='3'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pypianoroll import Multitrack, Track, load, parse\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lvl1 convolutions are shared between regions\n",
    "m = 128\n",
    "k = 512              # lvl1 nodes\n",
    "n_fft = 4096              # lvl1 receptive field\n",
    "window = 16384 # total number of audio samples?\n",
    "stride = 512\n",
    "batch_size = 100\n",
    "\n",
    "regions = 1 + (window - n_fft)//stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_segment(filepath,s, window):\n",
    "    sz_float = 4 # size of a float\n",
    "    epsilon = 10e-8 # fudge factor for normalization\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        filepath (str): path to the bin file\n",
    "        s (int): Start position of segment\n",
    "        window (int): how many data points to get\n",
    "    Returns:\n",
    "        audio waveform\n",
    "    \"\"\"\n",
    "\n",
    "    with open(filepath, 'rb') as f:\n",
    "        f.seek(s*sz_float, os.SEEK_SET)\n",
    "        x = np.fromfile(f, dtype=np.float32, count=int(scale*window))\n",
    "\n",
    "    x /= np.linalg.norm(x) + epsilon\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, avg=.9998):\n",
    "        super(Model, self).__init__()      \n",
    "        # Create filter windows\n",
    "        wsin, wcos = musicnet.create_filters(n_fft,k, low=50, high=6000,\n",
    "                                      windowing=\"hann\", freq_scale='log')\n",
    "        self.wsin = torch.Tensor(wsin)\n",
    "        self.wcos = torch.Tensor(wcos)               \n",
    "        # Creating Layers\n",
    "        \n",
    "        k_out = 128\n",
    "        k2_out = 256\n",
    "        self.CNN_freq = nn.Conv2d(1,k_out,\n",
    "                                kernel_size=(128,1),stride=(2,1))\n",
    "        self.CNN_time = nn.Conv2d(k_out,k2_out,\n",
    "                                kernel_size=(1,25),stride=(1,1))        \n",
    "        self.linear = torch.nn.Linear(k2_out*193, m, bias=False)\n",
    "\n",
    "        # Initialize weights\n",
    "            # Do something\n",
    "        \n",
    "    def forward(self,x):\n",
    "        zx = conv1d(x[:,None,:], self.wsin, stride=stride).pow(2) \\\n",
    "           + conv1d(x[:,None,:], self.wcos, stride=stride).pow(2) # shape = (batch, 512,25)\n",
    "        zx = torch.log(zx + 1e-12)\n",
    "        z2 = torch.relu(self.CNN_freq(zx.unsqueeze(1))) # Make channel as 1 (N,C,H,W) shape = [10, 128, 193, 25]\n",
    "        z3 = torch.relu(self.CNN_time(z2)) # shape = [10, 256, 193, 1]\n",
    "        y = self.linear(torch.relu(torch.flatten(z3,1)))\n",
    "        return y\n",
    "    \n",
    "class spectrograms_stft(torch.nn.Module):\n",
    "    def __init__(self, avg=.9998):\n",
    "        super(spectrograms_stft, self).__init__()\n",
    "        # Create filter windows for stft\n",
    "        wsin, wcos = musicnet.create_filters(n_fft,k, windowing=\"no\", freq_scale='linear')\n",
    "        self.wsin = torch.tensor(wsin, dtype=torch.float)\n",
    "        self.wcos = torch.tensor(wcos, dtype=torch.float)\n",
    "            \n",
    "        # Creating Layers\n",
    "        self.linear = torch.nn.Linear(regions*k, m, bias=False)\n",
    "        torch.nn.init.constant_(self.linear.weight, 0) # initialize\n",
    "        \n",
    "        self.avg = avg\n",
    "        \n",
    "    def forward(self,x):\n",
    "        zx = conv1d(x[:,None,:], self.wsin, stride=stride).pow(2) \\\n",
    "           + conv1d(x[:,None,:], self.wcos, stride=stride).pow(2) # Doing STFT by using conv1d\n",
    "        return self.linear(torch.log(zx + 10e-8).view(x.data.size()[0],regions*k))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = spectrograms_stft()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('../weights/spectrograms_stft'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def access_full(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        x = np.fromfile(f, dtype=np.float32)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io.wavfile import read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = read(filepath)[1]\n",
    "x1 = read('../data/test_data/2556.wav')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1987190,)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_piano_roll_from_wav(filepath, model, device, window=16384, stride=1000, offset=44100, count=7500, batch_size=500, m=128):\n",
    "    sf=4\n",
    "    x = read(filepath)[1]\n",
    "    if x.ndim==2:\n",
    "        x = x.mean(1) # convert stereo to mono\n",
    "    elif x.ndim>2:\n",
    "        print(\"the audio shape {} is not correct, please check and fix it\".format(x.shape))\n",
    "    \n",
    "    if stride == -1:\n",
    "        stride = (x.shape[0] - offset - int(sf*window))/(count-1)\n",
    "        stride = int(stride)\n",
    "        print(\"Number of stride = \", stride)\n",
    "    else:\n",
    "        count = (x.shape[0]- offset - int(sf*window))/stride + 1\n",
    "        count = int(count)\n",
    "        \n",
    "    X = np.zeros([count, window])\n",
    "    Y = np.zeros([count, m])    \n",
    "        \n",
    "    for i in range(count):\n",
    "        X[i,:] =  x[offset+i*stride:offset+i*stride+window]\n",
    "        #et_audio_segment(rec_id, offset+i*stride)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        Y_pred = torch.zeros([count,m])\n",
    "        for i in range(len(X)//batch_size):\n",
    "            print(f\"{i}/{(len(X)//batch_size)} batches\", end = '\\r')\n",
    "            X_batch = torch.tensor(X[batch_size*i:batch_size*(i+1)]).float().to(device)\n",
    "            Y_pred[batch_size*i:batch_size*(i+1)] = model(X_batch).cpu()\n",
    "    \n",
    "    return Y_pred, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_midi(Y_pred, path):\n",
    "    # Create a piano-roll matrix, where the first and second axes represent time\n",
    "    # and pitch, respectively, and assign a C major chord to the piano-roll\n",
    "    # Create a `pypianoroll.Track` instance\n",
    "    track = Track(pianoroll=Y_pred*127, program=0, is_drum=False,\n",
    "                  name='my awesome piano')   \n",
    "    multitrack = Multitrack(tracks=[track], tempo=60, beat_resolution=86)\n",
    "    multitrack.write(path)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = './'\n",
    "files = ['2.wav']\n",
    "filepath_list = [os.path.join(folder, i) for i in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/7 batches\r",
      "1/7 batches\r",
      "2/7 batches\r",
      "3/7 batches\r",
      "4/7 batches\r",
      "5/7 batches\r",
      "6/7 batches\r"
     ]
    }
   ],
   "source": [
    "for filepath in filepath_list:\n",
    "    Y_pred, x = get_piano_roll_from_wav(filepath, model, device,\n",
    "                                window=window, m=m, stride=512)\n",
    "    Yhatpred = Y_pred.cpu().numpy() > 0.4\n",
    "    export_midi(Yhatpred, './spectrograms_stft_{}_{}_Y_pred.mid'.format('transcription_',os.path.basename(filepath)[:-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
