{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, signal\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import conv1d\n",
    "import torchvision\n",
    "\n",
    "from time import time\n",
    "\n",
    "import musicnet\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'   # see issue #152\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='3'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pypianoroll import Multitrack, Track, load, parse\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lvl1 convolutions are shared between regions\n",
    "m = 128\n",
    "k = 512              # lvl1 nodes\n",
    "n_fft = 4096              # lvl1 receptive field\n",
    "window = 16384 # total number of audio samples?\n",
    "stride = 512\n",
    "batch_size = 100\n",
    "\n",
    "regions = 1 + (window - n_fft)//stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_segment(filepath,s, window):\n",
    "    sz_float = 4 # size of a float\n",
    "    epsilon = 10e-8 # fudge factor for normalization\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        filepath (str): path to the bin file\n",
    "        s (int): Start position of segment\n",
    "        window (int): how many data points to get\n",
    "    Returns:\n",
    "        audio waveform\n",
    "    \"\"\"\n",
    "\n",
    "    with open(filepath, 'rb') as f:\n",
    "        f.seek(s*sz_float, os.SEEK_SET)\n",
    "        x = np.fromfile(f, dtype=np.float32, count=int(scale*window))\n",
    "\n",
    "    x /= np.linalg.norm(x) + epsilon\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, avg=.9998):\n",
    "        super(Model, self).__init__()      \n",
    "        # Create filter windows\n",
    "        wsin, wcos = musicnet.create_filters(n_fft,k, low=50, high=6000,\n",
    "                                      windowing=\"hann\", freq_scale='log')\n",
    "        self.wsin = torch.Tensor(wsin)\n",
    "        self.wcos = torch.Tensor(wcos)               \n",
    "        # Creating Layers\n",
    "        \n",
    "        k_out = 128\n",
    "        k2_out = 256\n",
    "        self.CNN_freq = nn.Conv2d(1,k_out,\n",
    "                                kernel_size=(128,1),stride=(2,1))\n",
    "        self.CNN_time = nn.Conv2d(k_out,k2_out,\n",
    "                                kernel_size=(1,25),stride=(1,1))        \n",
    "        self.linear = torch.nn.Linear(k2_out*193, m, bias=False)\n",
    "\n",
    "        # Initialize weights\n",
    "            # Do something\n",
    "        \n",
    "    def forward(self,x):\n",
    "        zx = conv1d(x[:,None,:], self.wsin, stride=stride).pow(2) \\\n",
    "           + conv1d(x[:,None,:], self.wcos, stride=stride).pow(2) # shape = (batch, 512,25)\n",
    "        zx = torch.log(zx + 1e-12)\n",
    "        z2 = torch.relu(self.CNN_freq(zx.unsqueeze(1))) # Make channel as 1 (N,C,H,W) shape = [10, 128, 193, 25]\n",
    "        z3 = torch.relu(self.CNN_time(z2)) # shape = [10, 256, 193, 1]\n",
    "        y = self.linear(torch.relu(torch.flatten(z3,1)))\n",
    "        return y\n",
    "    \n",
    "class spectrograms_stft(torch.nn.Module):\n",
    "    def __init__(self, avg=.9998):\n",
    "        super(spectrograms_stft, self).__init__()\n",
    "        # Create filter windows for stft\n",
    "        wsin, wcos = musicnet.create_filters(n_fft,k, windowing=\"no\", freq_scale='linear')\n",
    "        self.wsin = torch.tensor(wsin, dtype=torch.float)\n",
    "        self.wcos = torch.tensor(wcos, dtype=torch.float)\n",
    "            \n",
    "        # Creating Layers\n",
    "        self.linear = torch.nn.Linear(regions*k, m, bias=False)\n",
    "        torch.nn.init.constant_(self.linear.weight, 0) # initialize\n",
    "        \n",
    "        self.avg = avg\n",
    "        \n",
    "    def forward(self,x):\n",
    "        zx = conv1d(x[:,None,:], self.wsin, stride=stride).pow(2) \\\n",
    "           + conv1d(x[:,None,:], self.wcos, stride=stride).pow(2) # Doing STFT by using conv1d\n",
    "        return self.linear(torch.log(zx + 10e-8).view(x.data.size()[0],regions*k))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./weights/translation_invariant_baseline'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def access_full(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        x = np.fromfile(f, dtype=np.float32)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_piano_roll(filepath, model, device, window=16384, stride=1000, offset=44100, count=7500, batch_size=500, m=128):\n",
    "    sf=4\n",
    "    x = access_full(filepath)\n",
    "    if stride == -1:\n",
    "        stride = (x.shape[0] - offset - int(sf*window))/(count-1)\n",
    "        stride = int(stride)\n",
    "        print(\"Number of stride = \", stride)\n",
    "    else:\n",
    "        count = (x.shape[0]- offset - int(sf*window))/stride + 1\n",
    "        count = int(count)\n",
    "        \n",
    "    X = np.zeros([count, window])\n",
    "    Y = np.zeros([count, m])    \n",
    "        \n",
    "    for i in range(count):\n",
    "        X[i,:] =  x[offset+i*stride:offset+i*stride+window]\n",
    "        #et_audio_segment(rec_id, offset+i*stride)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        Y_pred = torch.zeros([count,m])\n",
    "        for i in range(len(X)//batch_size):\n",
    "            print(f\"{i}/{(len(X)//batch_size)} batches\", end = '\\r')\n",
    "            X_batch = torch.tensor(X[batch_size*i:batch_size*(i+1)]).float().to(device)\n",
    "            Y_pred[batch_size*i:batch_size*(i+1)] = model(X_batch).cpu()\n",
    "    \n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_midi(Y_pred, path):\n",
    "    # Create a piano-roll matrix, where the first and second axes represent time\n",
    "    # and pitch, respectively, and assign a C major chord to the piano-roll\n",
    "    # Create a `pypianoroll.Track` instance\n",
    "    track = Track(pianoroll=Y_pred*127, program=0, is_drum=False,\n",
    "                  name='my awesome piano')   \n",
    "    multitrack = Multitrack(tracks=[track], tempo=60, beat_resolution=86)\n",
    "    multitrack.write(path)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_list = ['./data/test_data/1759.bin',\n",
    "                 './data/test_data/2106.bin',\n",
    "                 './data/test_data/2382.bin',\n",
    "                 './data/test_data/2556.bin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/25 batches\r"
     ]
    }
   ],
   "source": [
    "for filepath in filepath_list:\n",
    "    Y_pred = get_piano_roll(filepath, model, device,\n",
    "                                window=window, m=m, stride=512)\n",
    "    Yhatpred = Y_pred.cpu().numpy() > 0.4\n",
    "    export_midi(Yhatpred, './CNN_{}_{}_Y_pred.mid'.format('transcription_',os.path.basename(filepath)[:-4]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
